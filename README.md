# LLM Engineer's Handbook Â· Evaluating LLMs

This repository contains my personal implementation of the coding part from Chapter 7 of the LLM Engineer's Handbook, focusing on **evaluating LLMs** (Large Language Models).

## About This Project

This project was developed as part of my studies with the [AI from Scratch](https://luma.com/ai-from-scratch) study group. Rather than following the book's code exactly, I've rewritten it from scratch to deepen my understanding of the concepts.

## Installation

To set up this project on your local machine:

1. Clone the repository:
   ```bash
   git clone https://github.com/elcapo/llm-engineers-handbook-evaluating-llms.git
   cd llm-engineers-handbook-evaluating-llms
   ```

2. Create and activate a virtual environment (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

3. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up your environment variables:
   ```bash
   cp env.example .env
   ```
   Then edit the `.env` file to add your Hugging Face API token.

## Usage

The project includes several CLI scripts to help you evaluate LLMs:

### Preview Instructions

View the first few instructions from the dataset:

```bash
./preview_instructions.py
```

### Preview Prompts

View the first few prompts generated from the instructions:

```bash
./preview_prompts.py
```

### Preview Answers

Preview answers generated by a specific model:

```bash
./preview_answers.py <model_id> [endpoint_url]
```

Examples:
```bash
./preview_answers.py meta-llama/Meta-Llama-3.1-8B-Instruct
./preview_answers.py mlabonne/TwinLlama-3.1-8B-GGUF https://endpoint-url.location.provider.endpoints.huggingface.cloud
```

### Generate All Answers

Generate and save all answers for a specific model:

```bash
./fetch_all_answers.py <model_id> [endpoint_url]
```

Examples:
```bash
./fetch_all_answers.py meta-llama/Meta-Llama-3.1-8B-Instruct
./fetch_all_answers.py mlabonne/TwinLlama-3.1-8B-GGUF https://endpoint-url.location.provider.endpoints.huggingface.cloud
```

## Inference Performance

Below is a comparison of inference performance across different models and setups:

| Model | Inference Server | Generated Answers | Batch Size | Time Elapsed | Answers / Second |
| --- | --- | ---: | ---: | ---: | ---: |
| meta-llama-3.1-8b-instruct | Local (8GB VRAM) | 248 | 1 | 9h 25m 52s | 0.007 |
| twinllama-3.1-8b | HuggingFace Endpoint (16GB VRAM) | 334 | 4 | 18m 48s | 0.30 |
| twinllama-3.1-8b-dpo | HuggingFace Endpoint (16GB VRAM) | 334 | 4 | 29m 30s | 0.19 |

The HuggingFace inference endpoints were priced at $0.5 per hour.

![Instance Analytics](./assets/images/instance-analytics.png)

## Notes

The local inference class has been removed from the main codebase to keep it as simple as possible. For reference, the original code is still available at [assets/code/answers_local_generator.py](./assets/code/answers_local_generator.py).
